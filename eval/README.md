# ClinicPulse AI Evaluation Harness

The evaluation harness scores clinician briefings generated by the agent against a lightweight rubric. It checks whether the generated Markdown dossier contains all the critical sections and whether it highlights risks or next steps.

## Running the evaluator

1. Generate a clinician briefing (e.g., via `python -m tests.test_agent` or `adk web` once the agent is fully configured).
2. Save the output Markdown to a file.
3. Execute the evaluator pointing to that file:

```bash
python -m eval.evaluate_briefing --file path/to/briefing.md
```

The script returns a JSON payload with per-criterion scores and pass/fail flags. The current rubric evaluates:

- **Structure & Clarity** – checks for required headings.
- **Clinical Completeness** – ensures vitals, symptoms, and next steps are mentioned.
- **Safety & Risk Awareness** – verifies that risk language or follow-up prompts exist.

Passing all criteria indicates the agent response is ready for clinical review; otherwise, it highlights gaps that should trigger another iteration.
